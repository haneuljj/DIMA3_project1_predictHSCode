{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57be3a1-a55c-445a-ba4e-dd180af76887",
   "metadata": {},
   "source": [
    "# 84와 85 합친 df 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77440863-b547-4713-a39f-5f5b6cb0ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'hs code', 'product', 'translated_des',\n",
      "       'translated_reason'],\n",
      "      dtype='object') Index(['hs code', 'product', 'description', 'reason'], dtype='object') Index(['hs code', 'product ', 'description', 'reason'], dtype='object')\n",
      "\n",
      "Index(['Unnamed: 0', 'hs code', 'translated_product_290', 'translated_des',\n",
      "       'translated_reason'],\n",
      "      dtype='object') Index(['hs code', 'product', 'description', 'reason'], dtype='object') Index(['hs code', 'product', 'description', 'reason'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "origin_84 = pd.read_excel('./DATASET/fully_translated_df_with_hs.xlsx')\n",
    "less_84 = pd.read_excel('./DATASET/84_only_one.xlsx')\n",
    "none_84 = pd.read_excel('./DATASET/84_None.xlsx')\n",
    "origin_85 = pd.read_excel('./DATASET/fully_translated_df_hs85.xlsx')\n",
    "less_85 = pd.read_excel('./DATASET/85_less.xlsx')\n",
    "none_85 = pd.read_excel('./DATASET/85_none.xlsx')\n",
    "\n",
    "print(origin_84.columns, less_84.columns, none_84.columns)\n",
    "print()\n",
    "print(origin_85.columns, less_85.columns, none_85.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e361082-7466-4049-9e4b-7b535f78ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_84.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "origin_85.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e25d6af-e5d7-4f68-adbd-e736fd66d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_84.columns = ['hs code', 'product', 'description', 'reason']\n",
    "origin_85.columns = ['hs code', 'product', 'description', 'reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b3d203-1b1e-48b7-962c-41783829962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3868\n",
      "4835\n"
     ]
    }
   ],
   "source": [
    "df_84 = pd.concat([origin_84, less_84, none_84])\n",
    "df_85 = pd.concat([origin_85, less_85, none_85])\n",
    "\n",
    "print(len(df_84))\n",
    "print(len(df_85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89320601-7e00-4cb2-a2db-945ada8a98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_84['product'] = df_84['product'].astype(str)\n",
    "\n",
    "product84_des_list = []\n",
    "\n",
    "for name, des in zip(df_84['product'], df_84['description']):\n",
    "    product_des = name + des\n",
    "    product84_des_list.append(product_des)\n",
    "\n",
    "# product_des_list[0]\n",
    "product84_des_df = pd.DataFrame({'hs code':df_84['hs code'], 'description':product84_des_list, 'reason':df_84['reason']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1f7470-c37e-441e-b1ab-47159fdf1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_85['product'] = df_85['product'].astype(str)\n",
    "\n",
    "product85_des_list = []\n",
    "\n",
    "for name, des in zip(df_85['product'], df_85['description']):\n",
    "    product_des = name + des\n",
    "    product85_des_list.append(product_des)\n",
    "\n",
    "# product_des_list[0]\n",
    "product85_des_df = pd.DataFrame({'hs code':df_85['hs code'], 'description':product85_des_list, 'reason':df_85['reason']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2b3243-65ef-4fdc-99ae-a4019d84eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 3868 entries, 0 to 17\n",
      "Series name: reason\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "3867 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 60.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     ㅇ The tariff rate No. 8428 is classified as \"o...\n",
       "1     -The tariff rate table 16 Lord 3, “The combine...\n",
       "2     -The tariff rate table 8414 said, “Gastric pum...\n",
       "3     ㅇ The tariff rate table No. 16, the No. 2, sta...\n",
       "4     -Carcissization Table in the 16th parent No. 2...\n",
       "                            ...                        \n",
       "13    The applicable subheading for the oven/range h...\n",
       "14    The applicable subheading for the grease fitti...\n",
       "15    The applicable subheading for the dispense tip...\n",
       "16    84.53 Machinery for preparing, tanning or work...\n",
       "17    84.53 Machinery for preparing, tanning or work...\n",
       "Name: reason, Length: 3868, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product84_des_df['reason'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5ab7c6-8956-4228-95a2-bda2a00fc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6092\n",
      "2611\n",
      "14795\n",
      "14795\n",
      "2611\n",
      "2611\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_84 = product84_des_df['description'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "y_84 = product84_des_df['hs code']\n",
    "z_84 = product84_des_df['reason'] #.apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "\n",
    "x_85 = product85_des_df['description'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "y_85 = product85_des_df['hs code']\n",
    "z_85 = product85_des_df['reason'] #.apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "\n",
    "x = pd.concat([x_84, x_85])\n",
    "y = pd.concat([y_84, y_85])\n",
    "z = pd.concat([z_84, z_85])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=10)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "x_train = pd.concat([x_train, z])\n",
    "y_train = pd.concat([y_train, y])\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba728550-a7d9-4b87-a01b-8d26203c76b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# 특수문자\n",
    "string = string.punctuation\n",
    "# 불용어 + 특수문자 합친 리스트 생성\n",
    "for stri in string:\n",
    "    stopwords.append(stri)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # 어근 추출 객체 생성\n",
    "    lemmar = WordNetLemmatizer()\n",
    "\n",
    "    # 텍스트를 소문자로 변환 후 문장으로 토큰화\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "\n",
    "    for sentence in sentences:        \n",
    "        # 단어로 토큰화\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # 토큰화된 단어들의 어근 추출\n",
    "        token_list = [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return token_list\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=text_preprocessing, stop_words = stopwords)\n",
    "tfidf_vec.fit(x_train.values.astype('U'))\n",
    "x_train_tfidf_vec = tfidf_vec.transform(x_train.values.astype('U')) \n",
    "\n",
    "x_test_tfidf_vec = tfidf_vec.transform(x_test)\n",
    "y_test = y_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "881f9b3e-28c6-4e7f-8809-562d5f05e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.813\n",
      "정밀도: 0.808\n",
      "재현율: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "svc = LinearSVC(C=1)\n",
    "svc.fit(x_train_tfidf_vec,y_train)\n",
    "\n",
    "y_hat = svc.predict(x_test_tfidf_vec)\n",
    "\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')\n",
    "print(f'정밀도: {precision_score(y_test, y_hat, average=\"weighted\"):.3f}')\n",
    "print(f'재현율: {recall_score(y_test, y_hat, average=\"weighted\"):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3ce518e-7e3a-45e8-9933-524720a3c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.702\n",
      "정밀도: 0.697\n",
      "재현율: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(x_train_tfidf_vec,y_train)\n",
    "\n",
    "y_hat = lr.predict(x_test_tfidf_vec)\n",
    "\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')\n",
    "print(f'정밀도: {precision_score(y_test, y_hat, average=\"weighted\"):.3f}')\n",
    "print(f'재현율: {recall_score(y_test, y_hat, average=\"weighted\"):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10de15-b5cf-4350-9524-8ab5975bf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rmc =RandomForestClassifier(n_estimators=5000, max_depth=300)\n",
    "rmc.fit(x_train_tfidf_vec,y_train)\n",
    "\n",
    "y_hat = rmc.predict(x_test_tfidf_vec)\n",
    "\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')\n",
    "print(f'정밀도: {precision_score(y_test, y_hat, average=\"weighted\"):.3f}')\n",
    "print(f'재현율: {recall_score(y_test, y_hat, average=\"weighted\"):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d50f87-7e21-4ca9-84c5-f06d7c264707",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel('./DATASET/testing.xlsx')\n",
    "x_test = test_df['description']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
