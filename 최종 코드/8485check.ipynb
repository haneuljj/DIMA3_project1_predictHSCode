{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7427dfeb-e179-4129-af98-116f8edfc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099282f5-0697-4452-92f5-1b1590bdf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"G:\\내 드라이브\\DIMA3\\project\"\n",
    "os.chdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5d929e-30de-4a60-ad5a-9373c7fe84ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data84 = pd.read_excel('./DATASET/total data/84data_final.xlsx', index_col=0)\n",
    "data85 = pd.read_excel('./DATASET/total data/85data_final.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a10f75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data84[\"description\"].values.tolist()\n",
    "x.extend(data85[\"description\"].values.tolist())\n",
    "y = data84[\"hs code\"].values.tolist()\n",
    "y.extend(data85[\"hs code\"].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d1eabd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115417\n",
      "115417\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11b70c16-e3d5-4cbe-a959-a63f477877c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # pos_tag를 위한 데이터 다운로드\n",
    "\n",
    "# Initialize WordNet Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('x')\n",
    "stop_words.append('w')\n",
    "\n",
    "lemmatized_des = []\n",
    "for i in x:\n",
    "    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거\n",
    "    tokens = word_tokenize(i.lower())\n",
    "    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가\n",
    "\n",
    "    # 품사를 고려하여 단어의 원형 복원 (명사와 동사만 고려)\n",
    "    filtered_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')\n",
    "            filtered_tokens.append(lemma)\n",
    "        elif word.isalpha() and word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "    lemmatized_des.append(' '.join(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd95e57a-cf7c-412e-b2d3-b1fd63a9c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk import sent_tokenize\n",
    "# from nltk import word_tokenize\n",
    "# import nltk\n",
    "# import string\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# reason_stopwords = stopwords\n",
    "# print(len(reason_stopwords))\n",
    "\n",
    "# # stopwords = nltk.corpus.stopwords.words('english')\n",
    "# # 특수문자\n",
    "# string = string.punctuation\n",
    "# # 불용어 + 특수문자 합친 리스트 생성\n",
    "# for stri in string:\n",
    "#     reason_stopwords.append(stri)\n",
    "\n",
    "# token_list = None\n",
    "\n",
    "# def text_preprocessing(text):\n",
    "#     # 어근 추출 객체 생성\n",
    "#     lemmar = WordNetLemmatizer()\n",
    "\n",
    "#     global token_list\n",
    "#     # 텍스트를 소문자로 변환 후 문장으로 토큰화\n",
    "#     sentences = sent_tokenize(text.lower())\n",
    "\n",
    "#     for sentence in sentences:        \n",
    "#         # 단어로 토큰화\n",
    "#         tokens = word_tokenize(sentence)\n",
    "#         # 토큰화된 단어들의 어근 추출\n",
    "\n",
    "#         token_list = [lemmar.lemmatize(token) for token in tokens]\n",
    "#     return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a160850-b88d-4907-9b26-43fd59e7ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit(lemmatized_des)\n",
    "x_tfidf_vec = tfidf_vec.transform(lemmatized_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1afa36-1bad-467d-aea6-499dadbb1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_data = pd.read_excel(\"./DATASET/total data/test_data_total.xlsx\", index_col=0)\n",
    "# print(test_data.columns)\n",
    "y_test = test_data['hs code']\n",
    "x_test = test_data['name_des'].apply(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "295729b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     8479\n",
      "1     8486\n",
      "2     8473\n",
      "3     8486\n",
      "4     8462\n",
      "      ... \n",
      "95    8528\n",
      "96    8528\n",
      "97    8528\n",
      "98    8528\n",
      "99    8519\n",
      "Name: hs code, Length: 356, dtype: int64\n",
      "0     ELECTRIC MOTOR VEHICLE PARTS  undertow  electr...\n",
      "1     SECTIONS FOR SEMICONDUCTOR PRODUCTION CABLES u...\n",
      "2     PORTABLE TRIVETS CENTRIFUGAL FANS NOTEBOOK COM...\n",
      "3     SECTIONS OF STEEL FOR SEMICONDUCTOR PRODUCTION...\n",
      "4      BENDING DIGITAL CONTROL FOR PERFORMING THE SO...\n",
      "                            ...                        \n",
      "95    A    INCH    CM  COLOUR LCD MONITOR WITH AN AS...\n",
      "96    A         CM  COLOUR LCD MONITOR WITH A NATIVE...\n",
      "97    A      INCH     CM  LCD MONITOR IDEAL FOR ACCU...\n",
      "98    A       COLOUR LCD MONITOR  ASPECT RATIO      ...\n",
      "99    A BATTERY OPERATED PORTABLE MP  PLAYER  WITH B...\n",
      "Name: name_des, Length: 356, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf76dcb5-07f4-4b17-bb38-985e799e9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ssehn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # pos_tag를 위한 데이터 다운로드\n",
    "\n",
    "# Initialize WordNet Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('x')\n",
    "stop_words.append('w')\n",
    "\n",
    "lemmatized_xtest = []\n",
    "for i in x_test:\n",
    "    # 텍스트 전처리: 소문자 변환, 토큰화, 불용어 제거\n",
    "    tokens = word_tokenize(i.lower())\n",
    "    tagged_tokens = nltk.pos_tag(tokens)  # 단어 토큰에 품사 태깅 추가\n",
    "\n",
    "    # 품사를 고려하여 단어의 원형 복원 (명사와 동사만 고려)\n",
    "    filtered_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        if (tag.startswith('NN') or tag.startswith('VB')) and word not in stop_words:  # 명사 또는 동사인 경우\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n' if tag.startswith('NN') else 'v')\n",
    "            filtered_tokens.append(lemma)\n",
    "        elif word.isalpha() and word not in stop_words:\n",
    "            filtered_tokens.append(word)\n",
    "    lemmatized_xtest.append(' '.join(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9056a0f9-32ee-4d13-8edd-dbbba05e59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tfidf_vec = tfidf_vec.transform(lemmatized_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "039a09f3-e5e8-4fb8-8c11-4d1b75ce9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.854\n",
      "정밀도: 0.890\n",
      "재현율: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ssehn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# x_test_tfidf_vec = tfidf_vec.transform(x_test)\n",
    "\n",
    "svc_1 = LinearSVC(C=1, random_state=10)\n",
    "svc_1.fit(x_tfidf_vec,y)\n",
    "\n",
    "y_hat = svc_1.predict(x_test_tfidf_vec)\n",
    "\n",
    "# print(y_test)\n",
    "# print(y_hat)\n",
    "\n",
    "print(f'정확도: {accuracy_score(y_test, y_hat):.3f}')\n",
    "print(f'정밀도: {precision_score(y_test, y_hat, average=\"weighted\"):.3f}')\n",
    "print(f'재현율: {recall_score(y_test, y_hat, average=\"weighted\"):.3f}')\n",
    "report = classification_report(y_test, y_hat, output_dict=True)\n",
    "# classification_84_model_df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d492306c-a102-46b1-95ef-20749d64ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lsvc_0322_ssh.pkl', 'wb') as f:\n",
    "    pickle.dump(svc_1, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28406e2-831b-459e-b431-13a30f2d1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lsvc_tfidf_0322_ssh.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vec, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
